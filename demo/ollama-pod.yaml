apiVersion: v1
kind: Pod
metadata:
  name: ollama-pod
  labels:
    app: ollama
spec:
  containers:
  - name: ollama-deepseek
    image: ollama/ollama:latest
    imagePullPolicy: IfNotPresent
    command: ['sh', '-c', 'ollama start & sleep 20; ollama pull deepseek-r1 && tail -f /dev/null']
    ports:
    - containerPort: 11441
    env:
    - name: OLLAMA_MODEL_PATH
      value: /models/
    - name: OLLAMA_HOST
      value: "127.0.0.1:11441"
    - name: OLLAMA_KEEP_ALIVE
      value: "0"      
    resources:
      requests:
        memory: "512Mi"
        cpu: "250m"
      limits:
        memory: "1Gi"
        cpu: "500m"
    volumeMounts:
    - mountPath: /models
      name: model-storage
  - name: packet-kai8
    image: johncapobianco/my_app:latest
    imagePullPolicy: IfNotPresent
    ports:
      - containerPort: 8501
    resources:
      requests:
        memory: "1Gi"
        cpu: "1"
        nvidia.com/gpu: 1  # Request one GPU
      limits:
        memory: "1Gi"
        cpu: "1"
        nvidia.com/gpu: 1  # Limit to one GPU      
  - name: nginx
    image: nginx:latest
    imagePullPolicy: IfNotPresent
    ports:
    - containerPort: 80
    volumeMounts:
    - mountPath: /etc/nginx/nginx.conf
      name: nginx-config
      subPath: nginx.conf
  volumes:
  - name: model-storage
    emptyDir: {}
  - name: nginx-config
    configMap:
      name: nginx-config
  restartPolicy: Always