🟥 Stop K3s
bash
Copy
Edit
sudo systemctl stop k3s
🟩 Start K3s
bash
Copy
Edit
sudo systemctl start k3s
🔁 Restart K3s
bash
Copy
Edit
sudo systemctl restart k3s
📋 Check Status
bash
Copy
Edit
sudo systemctl status k3s

🔧 1. Install NVIDIA Drivers (if not already installed)
Check with:

bash
Copy
Edit
nvidia-smi
If it returns GPU info, you’re good. If not:

bash
Copy
Edit
sudo apt update
sudo apt install -y nvidia-driver-535
sudo reboot
🔧 2. Install NVIDIA Container Toolkit
This enables Docker (and containerd) to run GPU workloads.

bash
Copy
Edit
distribution=$(. /etc/os-release;echo $ID$VERSION_ID) && \
curl -s -L https://nvidia.github.io/libnvidia-container/gpgkey | sudo apt-key add - && \
curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \
  sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

sudo apt update
sudo apt install -y nvidia-container-toolkit
🔧 3. Configure containerd (used by K3s) to support NVIDIA runtime
Create or edit the file:

bash
Copy
Edit
sudo nano /etc/rancher/k3s/registries.yaml
Add:

yaml
Copy
Edit
configs:
  default:
    containerd:
      default_runtime_name: nvidia
      runtimes:
        nvidia:
          runtime_type: io.containerd.runc.v2
          options:
            BinaryName: "nvidia-container-runtime"
🔄 4. Restart K3s
Now that the runtime is in place, restart K3s:

bash
Copy
Edit
sudo systemctl restart k3s
Then check:

bash
Copy
Edit
sudo systemctl status k3s
You should see it running successfully.

🧪 Optional: Verify NVIDIA Runtime in a Pod
Create a simple pod spec to use GPU:

yaml
Copy
Edit
apiVersion: v1
kind: Pod
metadata:
  name: gpu-test
spec:
  containers:
  - name: cuda-container
    image: nvidia/cuda:11.0-base
    command: ["nvidia-smi"]
    resources:
      limits:
        nvidia.com/gpu: 1
Apply it:

bash
Copy
Edit
kubectl apply -f gpu-test.yaml
kubectl logs gpu-test






https://docs.lambdalabs.com/education/large-language-models/k8s-ollama-llama-3-2/
https://gist.github.com/clemenko/e3a823732c23813b43ac18fef0b24498


curl -s -L https://nvidia.github.io/nvidia-container-runtime/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-container-runtime/ubuntu22.04/nvidia-container-runtime.list |  sudo tee /etc/apt/sources.list.d/nvidia-container-runtime.list

wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.0-1_all.deb
sudo dpkg -i cuda-keyring_1.0-1_all.deb

sudo apt update
sudo apt install -y  nvidia-container-runtime

nvidia-container-cli info
nvidia-smi


curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE=644 sh -s - --default-runtime=nvidia --disable traefik



# Copy k3s config to your home directory
sudo cp /etc/rancher/k3s/k3s.yaml ~/.kube/config
sudo chown $USER:$USER ~/.kube/config
chmod 600 ~/.kube/config

export KUBECONFIG=~/.kube/config

sudo systemctl status k3s

curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 \
    && chmod 700 get_helm.sh \
    && ./get_helm.sh

kubectl create ns gpu-operator
kubectl label --overwrite ns gpu-operator pod-security.kubernetes.io/enforce=privileged

kubectl get nodes -o json | jq '.items[].metadata.labels | keys | any(startswith("feature.node.kubernetes.io"))'

helm repo add nvidia https://helm.ngc.nvidia.com/nvidia \
    && helm repo update

helm install --wait --generate-name \
    -n gpu-operator --create-namespace \
    nvidia/gpu-operator \
    --version=v24.9.2

